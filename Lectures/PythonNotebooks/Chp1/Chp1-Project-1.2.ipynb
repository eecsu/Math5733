{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1.2: *Linear Algebra*\n",
    "\n",
    "This project is intended to provide a quick refresher on some basic linear algebra concepts and how to implement them.\n",
    "Along the way, we will be using ``numpy`` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import matrix_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic concepts explored in this project\n",
    "\n",
    "Below, I summarize the basic concepts from this project and provide a few other noteworthy comments. Note that we could allow the vectors and matrices discussed below to have complex components, but we are limiting ourselves to real spaces for now. \n",
    "The extension to complex spaces is straightforward in most cases since $\\mathbb{C}$ is isometrically isomorphic to $\\mathbb{R}^2$ (i.e., they are \"basically the same\" and the more significant differences arise when considering behavior of functions on these spaces). \n",
    "\n",
    "* ***Linear Independence***. A set of vectors $V=\\{v_1, v_2, \\ldots, v_k\\}\\subset\\mathbb{R}^n$ is linearly independent if the only linear combination that produces the $n$-dimensional zero vector is the trivial linear combination (see (1.64) in the text). Otherwise, the set of vectors is a linearly dependent set.\n",
    "\n",
    "    * The ***span*** of a set of vectors $V$ is a subspace defined by the set of all possible linear combinations of vectors in $V$. \n",
    "\n",
    "    * A linearly independent spanning set for a subspace is called a ***basis***. \n",
    "\n",
    "    * We often use Fourier series as a basis for the solution space to a PDE in this course.\n",
    "    \n",
    "    \n",
    "* ***Singular and Nonsingular Matrices***. We only use these terms to refer to square matrices $A\\in\\mathbb{R}^{n\\times n}$. \n",
    "If matrix $A$ has a multiplicative inverse $A^{-1}$ so that $A^{-1}A= I$ where $I$ is the $n\\times n$ identity matrix, then $A$ is said to be nonsingular. \n",
    "Otherwise, we say that $A$ is singular.\n",
    "\n",
    "    * If $A$ is nonsingular, then $Ax=b$ can be solved for all $b\\in\\mathbb{R}^n$ and the solution is unique. \n",
    "    \n",
    "    * If $A$ is nonsingular, then $Ax=b$ either has no solution or an infinite number of solutions. It will have an infinite number of solutions if $b$ is in the span of the columns of $A$.\n",
    "    \n",
    "    * The ***rank*** of a matrix is the number of linearly independent columns (or rows) in the matrix (and the column and row ranks are exactly the same, which is why we just say rank in general). The rank of a nonsingular $n\\times n$ matrix is $n$.\n",
    "  \n",
    "  \n",
    "* ***Euclidean Inner Product and the Associated Norm***. We write the Euclidean inner product of two vectors $x,y\\in\\mathbb{R}^n$ as\n",
    "$$\n",
    "\\large (x,y) = \\sum_{j=1}^n x_jy_j,\n",
    "$$\n",
    "and the associated (Euclidean) norm is defined by \n",
    "$$\n",
    " \\large   ||x|| = (x,x)^{1/2}\n",
    "$$\n",
    "\n",
    "    * Two vectors $x$ and $y$ are ***orthogonal*** if $(x,y)=0$.\n",
    "    \n",
    "    * A set of vectors $V=\\{v_1, v_2, \\ldots, v_k\\}$ is an ***orthogonal set*** if $(v_i,v_j)=0$ for all $i\\neq j$. If, in addition, $||v_i||=1$ for all $i=1,2,\\ldots, k$, then the set is called ***orthonormal***.\n",
    "    \n",
    "    * Of all the properties that a norm satisfies, perhaps the two most important are the triangle inequality\n",
    "    $$\n",
    "        \\large ||x+y|| \\leq ||x|| + ||y||\n",
    "    $$\n",
    "    and the Cauchy-Schwarz inequality\n",
    "    $$\n",
    "        \\large (x,y)\\leq ||x||\\cdot||y||\n",
    "    $$\n",
    "\n",
    "\n",
    "* ***Eigenvalues and Eigenvectors***. This again only makes sense if we are discussing square matrices $A\\in\\mathbb{R}^{n\\times n}$ (the eigenvalues and eigenvectors may in fact be complex valued even if $A$ is not). We say that $\\lambda$ is an eigenvalue of $A$ if there exists a nonzero vector $x$ such that \n",
    "$$\n",
    "\\large Ax=\\lambda x.\n",
    "$$\n",
    "\n",
    "    * The span of all eigenvectors associated with an eigenvalue is called the ***eigenspace***. \n",
    "    \n",
    "    * Using eigenvectors to form a basis for a linear operator is a very convenient way to form a basis for the solution space that makes solving problems very straightforward. In particular, if an operator is self-adjoint (sometimes we say symmetric when referring to real-valued matrices, which just means $A=A^\\top$), then all the eigenvalues are real and the corresponding eigenvectors form an orthogonal set. In this case, it is very easy to solve problems involving the linear operator using the eigenvectors as a basis for the solution space. \n",
    "    \n",
    "    \n",
    "* ***Positive Definite Matrices***. If $A\\in\\mathbb{R}^{n\\times n}$ is symmetric and $x^\\top Ax>0$ for all nonzero $x\\in\\mathbb{R}^n$, then $A$ is said to be ***positive definite***. We often write SPD to denote a symmetric positive definite matrix. \n",
    "If $x^\\top Ax\\geq 0$ for all nonzero $x\\in\\mathbb{R}^n$, then $A$ is said to be ***positive semidefinite***.\n",
    "\n",
    "    * A SPD matrix is nonsingular.\n",
    "    \n",
    "    * A symmetric matrix is also positive definite if and only if all th e eigenvalues are real and strictly positive. If some of the eigenvalues are zero but all others are positive, then the symmetric matrix is positive semidefinite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (a)-(c): Linear Independence\n",
    "\n",
    "(a) Show that \n",
    "$$\n",
    "u_1 = \\left(\\begin{array}{c} 2 \\\\ -1 \\\\ 0\\end{array}\\right), \\ u_2 =  \\left(\\begin{array}{c} -1 \\\\ 2 \\\\ -1\\end{array}\\right), \\ u_3 =  \\left(\\begin{array}{c} 0 \\\\ -1 \\\\2\\end{array}\\right)\n",
    "$$\n",
    "form a linearly independent set of vectors.\n",
    "\n",
    "The matrix formed by these vectors looks a lot like a submatrix taken from discretizing a 1-D second-order derivative operatoar using centered finite differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -1  0]\n",
      " [-1  2 -1]\n",
      " [ 0 -1  2]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "u1 = np.array([2, -1, 0])\n",
    "u2 = np.array([-1, 2, -1])\n",
    "u3 = np.array([0, -1, 2])\n",
    "\n",
    "A = np.array([u1, u2, u3]).transpose()\n",
    "\n",
    "print A\n",
    "\n",
    "print matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Show that \n",
    "$$\n",
    " v_1 = \\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 3\\end{array}\\right), \\ v_2 = \\left(\\begin{array}{c} 4 \\\\ 5 \\\\ 6\\end{array}\\right), \\ v_3 = \\left(\\begin{array}{c} 7 \\\\ 8 \\\\ 9\\end{array}\\right)\n",
    "$$\n",
    "form a linearly dependent set of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "v3 = np.array([7, 8, 9])\n",
    "\n",
    "A = np.array([v1, v2, v3]).transpose()\n",
    "\n",
    "print A\n",
    "\n",
    "print matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Show that any collection of $n+1$ vectors in $\\mathbb{R}^n$ form a linearly dependent set.\n",
    "\n",
    "##### Proof:\n",
    "Let $e_i$ denote the standard basis vectors in $\\mathbb{R}^n$ for $i=1,2,\\ldots, n$. \n",
    "\n",
    "Let $V=\\{v_1, v_2, \\ldots, v_{n+1}\\}$ denote a set of $n+1$ vectors from $\\mathbb{R}^n$. Assume that this set is linearly independent.\n",
    "\n",
    "For $v_1$, there exists constants $\\alpha_i$, for $i=1,2,\\ldots, n$, such that\n",
    "$$\n",
    "   \\large v_1 = \\sum_{i=1}^n \\alpha_ie_i.\n",
    "$$\n",
    "Assume without loss of generality that $\\alpha_1\\neq 0$. \n",
    "Then, \n",
    "$$\n",
    "    \\large e_1 = \\frac{1}{\\alpha_1}\\left(v_1 - \\sum_{i=2}^n \\alpha_i e_i\\right).\n",
    "$$\n",
    "This implies that $\\{v_1, e_2, e_3, \\ldots, e_n\\}$ spans $\\mathbb{R}^n$. \n",
    "Thus, for $v_2$, there exists (new) constants $\\alpha_i$, for $i=1,2,\\ldots, n$, such that\n",
    "$$\n",
    "    \\large v_2 = \\alpha_1 v_1 + \\sum_{i=2}^n \\alpha_i e_i.\n",
    "$$\n",
    "Since the set $V$ is linearly independent, we must have that at least one $\\alpha_i\\neq 0$ for an $i=2,3,\\ldots, n$. \n",
    "Again, without loss of generality, assume that $\\alpha_2 \\neq 0$, and as before arrive at the conclusion that $\\{v_1, v_2, e_3, e_4, \\ldots, e_n\\}$ is a spanning set for $\\mathbb{R}^n$.\n",
    "\n",
    "We can repeat this argument $n$ times until we have replaced each $e_i$ with an associated $v_i$ so that $\\{v_1,v_2,\\ldots, v_n\\}$ forms a spanning set for $\\mathbb{R}^n$.\n",
    "However, this immediately implies that $v_{n+1}$ is a linear combination of these vectors, which contradics the assumption of linear independence. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (d)-(e): Rank this!\n",
    "\n",
    "(d) Show that $A_1$ and $A_2$ are invertible with the inverses as shown in the text and that $A_3$ is singular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -1  0]\n",
      " [-1  2 -1]\n",
      " [ 0 -1  2]]\n",
      "3\n",
      "[[ 0.75  0.5   0.25]\n",
      " [ 0.5   1.    0.5 ]\n",
      " [ 0.25  0.5   0.75]]\n",
      "[[ 1.          0.5         0.33333333]\n",
      " [ 0.5         0.33333333  0.25      ]\n",
      " [ 0.33333333  0.25        0.2       ]]\n",
      "3\n",
      "[[   9.  -36.   30.]\n",
      " [ -36.  192. -180.]\n",
      " [  30. -180.  180.]]\n",
      "[[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "A1 = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])\n",
    "A2 = np.array([[1, 1./2, 1./3], [1./2, 1./3, 1./4], [1./3, 1./4, 1./5]])\n",
    "A3 = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n",
    "\n",
    "print A1\n",
    "print matrix_rank(A1)\n",
    "print np.linalg.inv(A1)\n",
    "\n",
    "print A2\n",
    "print matrix_rank(A2)\n",
    "print np.linalg.inv(A2)\n",
    "\n",
    "print A3\n",
    "print matrix_rank(A3)\n",
    "#print np.linalg.inv(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(e) Solve the linear systems $A_1x_1=b_1$ and $A_2x_2=b_2$ where\n",
    "$$\n",
    "    b_1 = \\left(1, 2, 1\\right)^\\top \\ \\text{ and } \\ b_2 = \\left(-1, 2, -4\\right)^\\top.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  2.]\n",
      "[ -201.  1140. -1110.]\n",
      "[[ 1.  2.  1.]]\n",
      "[[-1.  2. -4.]]\n"
     ]
    }
   ],
   "source": [
    "b1 = np.array([1, 2, 1])\n",
    "b2 = np.array([-1, 2, -4])\n",
    "\n",
    "x1 = np.linalg.solve(A1, b1)\n",
    "print x1\n",
    "\n",
    "x2 = np.linalg.solve(A2, b2)\n",
    "print x2\n",
    "\n",
    "# check solutions\n",
    "print np.dot(A1, x1)\n",
    "print np.dot(A2, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Show that the rank of $A_1$, $A_2$, and $A_3$ is 3, 3, and 2, respectively.\n",
    "\n",
    "This was done in part (d) above.\n",
    "\n",
    "(g) Show that the inverse of a $2\\times 2$ matrix is the usual formula assuming the determinant is not zero.\n",
    "\n",
    "This is verified by straightforward matrix-matrix multiplication. \n",
    "The formula is also easily derived by augmenting the original $2\\times 2$ matrix with the $2\\times 2$ identity matrix and putting the augmented matrix into reduced row echelon form (the inverse will then be on the right of the augmented matrix). I am skipping this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (h)-(l): I know you're an inner product, but what am I?\n",
    "\n",
    "(h) Compute the inner products $(u_1,u_2)$, $(u_1,u_3)$, $(u_2, u_3)$, and $(v_1, v_3)$ using the vectors defined in parts (a) and (b) above. Also, compute the norms of $u_1$, $u_2$, $v_1$, and $v_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4\n",
      "1\n",
      "-4\n",
      "50\n",
      "2.2360679775\n",
      "2.44948974278\n",
      "3.74165738677\n",
      "13.9283882772\n"
     ]
    }
   ],
   "source": [
    "print np.vdot(u1, u2)\n",
    "print np.vdot(u1, u3)\n",
    "print np.vdot(u2, u3)\n",
    "print np.vdot(v1, v3)\n",
    "print np.linalg.norm(u1)\n",
    "print np.linalg.norm(u2)\n",
    "print np.linalg.norm(v1)\n",
    "print np.linalg.norm(v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Prove the Theorem of Pythagoras. \n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Let $x,y\\in\\mathbb{R}^n$ be orthogonal. Then,\n",
    "\\begin{eqnarray}\n",
    "   ||x+y||^2 &=& (x+y,x+y) \\\\\n",
    "              &=& (x,x) + \\underbrace{(x,y)}_{=0} + \\underbrace{(y,x)}_{=0} + (y,y) \\\\\n",
    "              &=& (x,x) + (y,y) \\\\\n",
    "              &=& ||x||^2 + ||y||^2 \\ \\Box\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(j) Show that a set of othonormal vectors forms a linearly independent set. \n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Let $V=\\{v_1,v_2,\\ldots, v_k\\}\\subset\\mathbb{R}^n$ be a set of orthonormal vectors and let $\\alpha_i$ for $1\\leq i\\leq k$ be any constants such that\n",
    "$$\n",
    "    \\alpha_1 v_1 + \\alpha_2 v_2 + \\cdots + \\alpha_k v_k = 0.\n",
    "$$\n",
    "Consider any $1\\leq j\\leq k$, take the inner product of both sides of the above equation with $v_j$. \n",
    "The orthormality of the vectors implies\n",
    "$$\n",
    "    \\alpha_j = 0.\n",
    "$$\n",
    "Since the $j\\in\\{1,2,\\ldots, k\\}$ was arbitrary, this shows that the only linear combination of the vectors in $V$ that produces the zero vector is in fact the trivial linear combination. \n",
    "Thus, the vectors $V$ are linearly independent. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(k) Show that the usual basis of $\\mathbb{R}^n$ forms an orthonormal set.\n",
    "\n",
    "This is clear from inspection. \n",
    "\n",
    "(l) Basically you are asked to show that an orthonormal set in $\\mathbb{R}^n$ is a basis for $\\mathbb{R}^n$ and that the (unique) coefficients we use to write a vector $z\\in\\mathbb{R}^n$ as a linear combination of this orthonormal set are easily determined. \n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Let $Y=\\{y_1,y_2,\\ldots, y_n\\}\\subset\\mathbb{R}^n$ be an orthonormal set.\n",
    "Then, by part (j), this is a linearly independent set of $n$ vectors.\n",
    "Thus, it is a basis for $\\mathbb{R}^n$ by a standard result in linear algebra.\n",
    "Finally, for any $z\\in\\mathbb{R}^n$, there exists constants $\\{c_1,c_2,\\ldots, c_n\\}\\subset\\mathbb{R}$ such that\n",
    "$$\n",
    "    z = \\sum_{j=1}^n c_jy_j.\n",
    "$$\n",
    "These constants can easily be determined by taking the inner product of both sides of the equation with respect to $y_k$ for $1\\leq k\\leq n$ and exploiting the orthormality of the set to reveal that\n",
    "$$\n",
    "    (z, y_k) = c_k \\ \\forall \\ 1\\leq k\\leq n. \\ \\Box\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (m)-(r): Eigenwho? Eigenwhat? \n",
    "\n",
    "(m) Find some eigenvalues and eigenvectors for matrices $A_1$ above and also \n",
    "$$\n",
    "\\large A_4 = \\left(\\begin{array}{cc}\n",
    "                     2 & -1 \\\\\n",
    "                     -1 & 2\n",
    "                   \\end{array}\\right), \\ \\text{ and } \\\n",
    "       A_5 = \\left(\\begin{array}{cc}\n",
    "                     1 & -1 \\\\\n",
    "                     -1 & 1\n",
    "                   \\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eig. values of A1, \n",
      "[ 3.41421356  2.          0.58578644]\n",
      "\n",
      "Eig. vectors of A1, \n",
      "[[ -5.00000000e-01  -7.07106781e-01   5.00000000e-01]\n",
      " [  7.07106781e-01   4.05405432e-16   7.07106781e-01]\n",
      " [ -5.00000000e-01   7.07106781e-01   5.00000000e-01]]\n",
      "\n",
      "Eig. values of A4, \n",
      "[ 3.  1.]\n",
      "\n",
      "Eig. vectors of A4, \n",
      "[[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "\n",
      "Eig. values of A5, \n",
      "[ 2.  0.]\n",
      "\n",
      "Eig. vectors of A5, \n",
      "[[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "A4 = np.array([[2, -1], [-1, 2]])\n",
    "A5 = np.array([[1, -1], [-1, 1]])\n",
    "\n",
    "eig_vals1, eig_vecs1 = np.linalg.eig(A1)\n",
    "eig_vals4, eig_vecs4 = np.linalg.eig(A4)\n",
    "eig_vals5, eig_vecs5 = np.linalg.eig(A5)\n",
    "\n",
    "print 'Eig. values of A1, \\n', eig_vals1\n",
    "print '\\nEig. vectors of A1, \\n', eig_vecs1\n",
    "\n",
    "print '\\nEig. values of A4, \\n', eig_vals4\n",
    "print '\\nEig. vectors of A4, \\n', eig_vecs4\n",
    "\n",
    "print '\\nEig. values of A5, \\n', eig_vals5\n",
    "print '\\nEig. vectors of A5, \\n', eig_vecs5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(n) Verify that the eigenvectors for $A_4$ and $A_5$ are orthogonal. \n",
    "\n",
    "This should be the case since these matrices are clearly symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print np.inner(eig_vecs4[:,0], eig_vecs4[:,1])\n",
    "print np.inner(eig_vecs5[:,0], eig_vecs5[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(o) Suppose $\\lambda$ and $x$ are an eigenvalue/eigenvector pair associated with a nonsingular matrix $A\\in\\mathbb{R}^{n\\times n}$ and $\\alpha_1$ is a scalar, show that $\\mu_1=1+\\alpha_1\\lambda$ and $x$ are an eigenvalue/eigenvector pair for the matrix $B=I+\\alpha_1 A$. \n",
    "\n",
    "***Proof:***\n",
    "\\begin{eqnarray}\n",
    "    B_1x & = & (I+\\alpha_1A)x \\\\\n",
    "         & = & Ix + \\alpha_1 Ax \\\\\n",
    "         & = & x + \\alpha_1 \\lambda x \\\\\n",
    "         & = & (1+\\alpha_1\\lambda)x \\\\\n",
    "         & = & \\mu_1 x. \\ \\Box\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(p), (q), and (r). ***Skipped.*** These are just generalizations of (o) or require very similar arguments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (s)-(v): I am Positively Definitely done with this project.\n",
    "\n",
    "(s) Show that $A_1$ and $A_4$ are symmetric and positive definite and the matrix $A_5$ is symmetric and positive semidefinite.\n",
    "\n",
    "The symmetry is obvious from inspection, but it shown below anyway. \n",
    "\n",
    "The positive definteness or semidefiniteness is clear from the eigenvalues computed above and the standard if-and-only-if theorems regarding these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[0 0]\n",
      " [0 0]]\n",
      "[[0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print A1 - np.transpose(A1)\n",
    "print A4 - np.transpose(A4)\n",
    "print A5 - np.transpose(A5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(t) Show that a sum of SPD matrices is also SPD.\n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Suppose $A,B\\in\\mathbb{R}^{n\\times n}$ are SPD and let $C=A+B$.\n",
    "Then, \n",
    "$$\n",
    "    C^\\top = (A+B)^\\top = A^\\top + B^\\top = A+B = C,\n",
    "$$\n",
    "so $C$ is symmetric.\n",
    "Let $x\\in\\mathbb{R}^n$ be nonzero.\n",
    "Then,\n",
    "$$\n",
    "    x^\\top Cx = x^\\top(A+B)x = x^\\top Ax + x^\\top Bx > 0,\n",
    "$$\n",
    "so $C$ is positive definite. $\\Box$\n",
    "\n",
    "\n",
    "(u) Let $A\\in\\mathbb{R}^{n\\times n}$ be nonsingular and $B=A^\\top A$. \n",
    "Show that $B$ is SPD.\n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Let $A\\in\\mathbb{R}^{n\\times n}$ be nonsingular and $B=A^\\top A$. Then,\n",
    "$$\n",
    "    B^\\top = (A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A = B,\n",
    "$$\n",
    "so $B$ is symmetric.\n",
    "Let $x\\in\\mathbb{R}^n$ be nonzero, then\n",
    "$$\n",
    "    x^\\top B x = x^\\top A^\\top A x = (Ax, Ax) = ||Ax||^2.\n",
    "$$\n",
    "Since $A$ is nonsingular and $x\\neq 0$, then $Ax\\neq 0$, so $||Ax||^2>0$. \n",
    "Thus, \n",
    "$$\n",
    "    x^\\top B x > 0, \n",
    "$$\n",
    "which implies $B$ is positive definite. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(v) Skipped. This proof is straightofrward like the previous two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
